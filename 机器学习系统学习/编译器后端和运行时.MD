# 7.1概述
    编译器前端主要将用户代码进行解析翻译得到计算图IR，并对其进行设备信息无关的优化，此时的优化并不考虑程序执行的底层硬件信息。编译器后端的主要职责是对前端下发的IR做进一步的计算图优化，让其更加贴合硬件，并为IR中的计算节点选择在硬件上执行的算子，然后为每个算子的输入输出分配硬件内存，最终生成一个可以在硬件上执行的任务序列。


    前端IR是通过解析用户代码生成，属于高层次抽象，隐藏一些底层运行的细节信息

    后端优化基本流程 计算图优化(算子融合，复杂算子组合表达)----------->>算子选择（优化的IR图后选取最合适的目标设备算子的过程，IR往往可以映射成多种不同的硬件算子，如何组成高效算子，是算子选择核心问题）。算子选择本质上是一个模式匹配问题（编译器一般为一个IR节点有多个候选算子，高层次IR------》底层IR）------------------------》遍历算子序列，为每个算子分配相应的输入输出内存，算子加载到设备上执行计算。

## 7.1.1计算图优化
    优化内存IO为例：计算密集型，访存密集型算子，如Conv + Relu:二者融合来进行计算;算子拆解、算子聚合、算子重建减少内存访问.
### 7.1.1.1 特定硬件优化

### 7.1.1.2 数据排布格式的限制
    内存中都是按行存储的数据格式
    RRRBBBGGG NCHW
## 7.1.2算子选择

## 7.1.3内存分配



## 7.1.4计算调度执行
    经过算子选择与内存分配之后，计算任务可以通过运行时完成计算的调度与在硬件上的执行。根据是否将算子编译为计算图，计算的调度可以分为单算子调度与计算图调度两种方式。而根据硬件提供的能力差异，计算图的执行方式又可以分为逐算子下发执行的交互式执行以及将整个计算图或者部分子图一次性下发到硬件的下沉式执行两种模式。

## 7.1.5. 算子编译器
    作为AI编译器中一个重要组成部分，算子编译器把单个简单或复杂的算子经过表达和优化后编译为一个单独的可执行文件。目前业界面对算子编译器仍有许多有趣的问题尚未得出明确结论，相关的处理逻辑与方法也尚未收敛,如何通过算子编译器进行性能优化？算子编译器如何兼容不同体系结构特点的芯片？面对输入Python代码的灵活性以及神经网络训练时动态性的情况，该如何充分将这些完美表达出来？